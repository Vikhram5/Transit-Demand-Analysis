{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ee6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# date=input(\"Enter the date: \")\n",
    "# df=pd.read_csv(f'./Latest_ETM_Data/Final_Ticket_data_2024-07-{date}.csv')\n",
    "# folder_name_odm=\"ODM_Files\"\n",
    "# folder_name_od=\"UP_DOWN\"\n",
    "\n",
    "\n",
    "# all_zonal_centroids = range(1, 3039)\n",
    "# all_pairs = pd.MultiIndex.from_product(\n",
    "#     [all_zonal_centroids, all_zonal_centroids],\n",
    "#     names=['source_zonal_centroid_number', 'destination_zonal_centroid_number']\n",
    "# )\n",
    "\n",
    "# grouped = df.groupby(['source_zonal_centroid_number', 'destination_zonal_centroid_number'])['passenger_count'].sum()\n",
    "# od_matrix = grouped.reindex(all_pairs, fill_value=0).unstack(fill_value=0)\n",
    "\n",
    "# od_matrix['Total Passengers'] = od_matrix.sum(axis=1)\n",
    "# od_matrix.to_csv(f'{folder_name_odm}/ODM-07-{date}.csv', index=False)\n",
    "\n",
    "\n",
    "# cols=['Trip_Origin','Trip_Destination','Direction_route']\n",
    "# df=df[cols]\n",
    "# result = df[df['Direction_route'].isna()].drop_duplicates()\n",
    "# result.to_csv(f'{folder_name_od}/UP_DOWN_{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909889d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fe3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8074f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3337b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da91ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vikhram\n",
    "\n",
    "# import pandas as pd\n",
    "# folder_name=\"Latest_ETM_Data\"\n",
    "# folder2_name=\"Match\"\n",
    "\n",
    "# #ETM file for a specific date\n",
    "# date=input(\"Enter the date: \")\n",
    "# ticket_df=pd.read_csv(f'ETM_Data/Ticket-Data-2024-07-{date}.csv')\n",
    "# ticket_df.head()\n",
    "\n",
    "\n",
    "# #Direction Route - UP and DOWN \n",
    "# route_df=pd.read_csv('final_routes_directions.csv')\n",
    "# ticket_df = ticket_df.merge(route_df[['Trip_Origin', 'Trip_Destination', 'Direction_Route']], \n",
    "#                             on=['Trip_Origin', 'Trip_Destination'], \n",
    "#                             how='left')\n",
    "\n",
    "# ticket_df.rename(columns={'Direction_Route': 'Direction_route'}, inplace=True)\n",
    "\n",
    "# ticket_df['source']=ticket_df['source'].str.upper().str.split().str.join(' ')\n",
    "# ticket_df['destination']=ticket_df['destination'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "# stops_df=pd.read_csv('latest_stops_centroid_numbering.csv')\n",
    "# stops_df['DIRECTION'] = stops_df['DIRECTION'].str.upper().str.split().str.join(' ')\n",
    "# stops_df['STOP NAME'] = stops_df['Stop Name'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #original and renamed stops mapping\n",
    "# final_stops_df= pd.read_csv('stops_mapping.csv')\n",
    "# mapping_stops = final_stops_df.set_index('Stops')['Renamed Stops'].to_dict()\n",
    "\n",
    "# ticket_df['source'] = ticket_df['source'].map(mapping_stops).fillna(ticket_df['source'])\n",
    "# ticket_df['destination'] = ticket_df['destination'].map(mapping_stops).fillna(ticket_df['destination'])\n",
    "\n",
    "\n",
    "# stops_df_unique = stops_df.drop_duplicates(subset=['STOP NAME', 'DIRECTION'])\n",
    "\n",
    "# def merge_and_insert(ticket_df, stops_df, stop_column, direction_column):\n",
    "#     ticket_df = ticket_df.merge(\n",
    "#         stops_df[['STOP.NO', 'STOP NAME', 'DIRECTION']],\n",
    "#         how='left',\n",
    "#         left_on=[stop_column, direction_column],\n",
    "#         right_on=['STOP NAME', 'DIRECTION']\n",
    "#     )\n",
    "\n",
    "#     # Rename and extract necessary columns\n",
    "#     stop_no_column = f\"{stop_column}_stop_no\"\n",
    "#     ticket_df = ticket_df.rename(columns={'STOP.NO': stop_no_column}).drop(columns=['STOP NAME', 'DIRECTION'])\n",
    "    \n",
    "#     stop_no = ticket_df.pop(stop_no_column)\n",
    "#     stop_index = ticket_df.columns.get_loc(stop_column) + 1  \n",
    "#     ticket_df.insert(stop_index, stop_no_column, stop_no)\n",
    "\n",
    "#     return ticket_df\n",
    "\n",
    "# ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'source', 'Direction_route')\n",
    "# ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'destination', 'Direction_route')\n",
    "\n",
    "# # Filter rows where DIRECTION is NaN\n",
    "# nodirection_stops= stops_df[stops_df['DIRECTION'].isna()]\n",
    "# nodirection_stops\n",
    "\n",
    "# # Unique stops without considering DIRECTION\n",
    "# nodirection_stops_unique = stops_df[stops_df['DIRECTION'].isna()].drop_duplicates(subset=['STOP NAME'])\n",
    "\n",
    "# def update_stop_no(ticket_df, stops_df, stop_column, stop_no_column):\n",
    "#     # Merge based on 'STOP NAME' only\n",
    "#     updated_df = ticket_df.merge(\n",
    "#         stops_df[['STOP.NO', 'STOP NAME']],\n",
    "#         how='left',\n",
    "#         left_on=stop_column,\n",
    "#         right_on='STOP NAME'\n",
    "#     )\n",
    "    \n",
    "#     # Only update rows where stop_no_column is NaN\n",
    "#     ticket_df.loc[ticket_df[stop_no_column].isna(), stop_no_column] = updated_df['STOP.NO']\n",
    "#     return ticket_df\n",
    "\n",
    "# ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'source', 'source_stop_no')\n",
    "# ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'destination', 'destination_stop_no')\n",
    "\n",
    "\n",
    "# ticket_df = ticket_df.loc[:, ~ticket_df.columns.str.contains('^Unnamed')]\n",
    "# ticket_df.to_csv(f'Temporary_File_{date}.csv')\n",
    "\n",
    "# df=pd.read_csv(f'Temporary_File_{date}.csv',index_col=False)\n",
    "# df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# source_list=ticket_df['source'].str.upper().unique()\n",
    "# destination_list=ticket_df['destination'].str.upper().unique()\n",
    "\n",
    "# stop_list=stops_df['STOP NAME'].unique()\n",
    "# source_arr=[]\n",
    "# source_count=0\n",
    "\n",
    "\n",
    "# combined_unmatched_stops=source_arr+dest_arr\n",
    "# unique_unmatched_stops=list(set(combined_unmatched_stops))\n",
    "# unique_unmatched_stops = [name for name in unique_unmatched_stops if pd.notna(name) and name.strip()]\n",
    "\n",
    "# unique_unmatched_df = pd.DataFrame(unique_unmatched_stops, columns=['unique_unmatched_stops'])\n",
    "# unique_unmatched_df = unique_unmatched_df.sort_values(by='unique_unmatched_stops').reset_index(drop=True)\n",
    "# unique_unmatched_df.to_csv(f'{folder2_name}/unique_unmatched_stops_{date}.csv', index=False)\n",
    "\n",
    "# stops_df=pd.read_csv('latest_stops_centroid_numbering.csv')\n",
    "# stops_df['STOP NAME'] = stops_df['Stop Name'].str.upper().str.split().str.join(' ')\n",
    "# stops_df['DIRECTION'] = stops_df['DIRECTION'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "\n",
    "# stops_df_unique = stops_df[['STOP NAME', 'CENTROID NUMBERING']].drop_duplicates(subset='STOP NAME')\n",
    "\n",
    "# df = df.merge(stops_df_unique, \n",
    "#               left_on='source', \n",
    "#               right_on='STOP NAME', \n",
    "#               how='left')\n",
    "\n",
    "# df = df.rename(columns={'CENTROID NUMBERING': 'source_zonal_centroid_number'})\n",
    "# df = df.drop(columns=['STOP NAME'])\n",
    "# cols = df.columns.tolist()\n",
    "# source_idx = cols.index('source_stop_no')\n",
    "# cols.insert(source_idx + 1, cols.pop(cols.index('source_zonal_centroid_number')))\n",
    "# df = df[cols]\n",
    "\n",
    "# stops_df_unique = stops_df[['STOP NAME', 'CENTROID NUMBERING']].drop_duplicates(subset='STOP NAME')\n",
    "\n",
    "# df = df.merge(stops_df_unique, \n",
    "#               left_on='destination', \n",
    "#               right_on='STOP NAME', \n",
    "#               how='left')\n",
    "\n",
    "# df = df.rename(columns={'CENTROID NUMBERING': 'destination_zonal_centroid_number'})\n",
    "# df = df.drop(columns=['STOP NAME'])\n",
    "\n",
    "# cols = df.columns.tolist()\n",
    "# destination_idx = cols.index('destination_stop_no')\n",
    "# cols.insert(destination_idx + 1, cols.pop(cols.index('destination_zonal_centroid_number')))\n",
    "# df = df[cols]\n",
    "# df.to_csv(f'{folder_name}/Final_Ticket_data_2024-07-{date}.csv', index=False)\n",
    "# print(f\"File saved to {folder_name}/Final_Ticket_data_2024-07-{date}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8c7bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'depot_code', 'depot', 'fleet_no', 'route_service_type',\n",
       "       'trip_no', 'ticket_no', 'date', 'time', 'ticket_type', 'source',\n",
       "       'destination', 'passenger_count', 'luggage_count', 'passenger_amount',\n",
       "       'luggage_amount', 'driver_no', 'conductor_no', 'waybill_no',\n",
       "       'schedule_no', 'payment_type', 'direction', 'Trip_Origin',\n",
       "       'Trip_Destination', 'total_ticket_amount', 'KM', 'duty_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ticket_df = pd.read_csv(f'ETM_Data/Ticket-Data-2024-07-01.csv')\n",
    "ticket_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8570d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 2024-07-01...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-01.csv\n",
      "Processing for 2024-07-02...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-02.csv\n",
      "Processing for 2024-07-03...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-03.csv\n",
      "Processing for 2024-07-04...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-04.csv\n",
      "Processing for 2024-07-05...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-05.csv\n",
      "Processing for 2024-07-06...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-06.csv\n",
      "Processing for 2024-07-07...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-07.csv\n",
      "Processing for 2024-07-08...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-08.csv\n",
      "Processing for 2024-07-09...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-09.csv\n",
      "Processing for 2024-07-10...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-10.csv\n",
      "Processing for 2024-07-11...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-11.csv\n",
      "Processing for 2024-07-12...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-12.csv\n",
      "Processing for 2024-07-13...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-13.csv\n",
      "Processing for 2024-07-14...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-14.csv\n",
      "Processing for 2024-07-15...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-15.csv\n",
      "Processing for 2024-07-16...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-16.csv\n",
      "Processing for 2024-07-17...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-17.csv\n",
      "Processing for 2024-07-18...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-18.csv\n",
      "Processing for 2024-07-19...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-19.csv\n",
      "Processing for 2024-07-20...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-20.csv\n",
      "Processing for 2024-07-21...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-21.csv\n",
      "Processing for 2024-07-22...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-22.csv\n",
      "Processing for 2024-07-23...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-23.csv\n",
      "Processing for 2024-07-24...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-24.csv\n",
      "Processing for 2024-07-25...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-25.csv\n",
      "Processing for 2024-07-26...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-26.csv\n",
      "Processing for 2024-07-27...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-27.csv\n",
      "Processing for 2024-07-28...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-28.csv\n",
      "Processing for 2024-07-29...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-29.csv\n",
      "Processing for 2024-07-30...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-30.csv\n",
      "Processing for 2024-07-31...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-31.csv\n"
     ]
    }
   ],
   "source": [
    "#for all the dates from 01 to 31 - latest etm files \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directories\n",
    "folder_name = \"Latest_ETM_Data\"\n",
    "folder2_name = \"Match\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "os.makedirs(folder2_name, exist_ok=True)\n",
    "\n",
    "# Loop through all dates in the month\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"  # Format date as two digits (e.g., 01, 02, ..., 31)\n",
    "    try:\n",
    "        print(f\"Processing for 2024-07-{date_str}...\")\n",
    "        \n",
    "        # Load Ticket data for the current date\n",
    "        ticket_df = pd.read_csv(f'ETM_Data/Ticket-Data-2024-07-{date_str}.csv')\n",
    "\n",
    "        # Load Direction Route data\n",
    "        route_df = pd.read_csv('final_routes_directions.csv')\n",
    "        ticket_df = ticket_df.merge(route_df[['Trip_Origin', 'Trip_Destination', 'Direction_Route']], \n",
    "                                    on=['Trip_Origin', 'Trip_Destination'], \n",
    "                                    how='left')\n",
    "        ticket_df.rename(columns={'Direction_Route': 'Direction_route'}, inplace=True)\n",
    "\n",
    "        # Process stop names and directions\n",
    "        ticket_df['source'] = ticket_df['source'].str.upper().str.split().str.join(' ')\n",
    "        ticket_df['destination'] = ticket_df['destination'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "        stops_df = pd.read_csv('latest_stops_centroid_numbering.csv')\n",
    "        stops_df['DIRECTION'] = stops_df['DIRECTION'].str.upper().str.split().str.join(' ')\n",
    "        stops_df['STOP NAME'] = stops_df['Stop Name'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "        # Map original and renamed stops\n",
    "        final_stops_df = pd.read_csv('stops_mapping.csv')\n",
    "        mapping_stops = final_stops_df.set_index('Stops')['Renamed Stops'].to_dict()\n",
    "\n",
    "        ticket_df['source'] = ticket_df['source'].map(mapping_stops).fillna(ticket_df['source'])\n",
    "        ticket_df['destination'] = ticket_df['destination'].map(mapping_stops).fillna(ticket_df['destination'])\n",
    "\n",
    "        stops_df_unique = stops_df.drop_duplicates(subset=['STOP NAME', 'DIRECTION'])\n",
    "\n",
    "        # Define merge_and_insert function\n",
    "        def merge_and_insert(ticket_df, stops_df, stop_column, direction_column):\n",
    "            ticket_df = ticket_df.merge(\n",
    "                stops_df[['STOP.NO', 'STOP NAME', 'DIRECTION']],\n",
    "                how='left',\n",
    "                left_on=[stop_column, direction_column],\n",
    "                right_on=['STOP NAME', 'DIRECTION']\n",
    "            )\n",
    "            stop_no_column = f\"{stop_column}_stop_no\"\n",
    "            ticket_df = ticket_df.rename(columns={'STOP.NO': stop_no_column}).drop(columns=['STOP NAME', 'DIRECTION'])\n",
    "            stop_no = ticket_df.pop(stop_no_column)\n",
    "            stop_index = ticket_df.columns.get_loc(stop_column) + 1  \n",
    "            ticket_df.insert(stop_index, stop_no_column, stop_no)\n",
    "            return ticket_df\n",
    "\n",
    "        # Insert stop numbers\n",
    "        ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'source', 'Direction_route')\n",
    "        ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'destination', 'Direction_route')\n",
    "\n",
    "        # Process unmatched stops\n",
    "        nodirection_stops_unique = stops_df[stops_df['DIRECTION'].isna()].drop_duplicates(subset=['STOP NAME'])\n",
    "\n",
    "        def update_stop_no(ticket_df, stops_df, stop_column, stop_no_column):\n",
    "            updated_df = ticket_df.merge(\n",
    "                stops_df[['STOP.NO', 'STOP NAME']],\n",
    "                how='left',\n",
    "                left_on=stop_column,\n",
    "                right_on='STOP NAME'\n",
    "            )\n",
    "            ticket_df.loc[ticket_df[stop_no_column].isna(), stop_no_column] = updated_df['STOP.NO']\n",
    "            return ticket_df\n",
    "\n",
    "        ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'source', 'source_stop_no')\n",
    "        ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'destination', 'destination_stop_no')\n",
    "\n",
    "        ticket_df = ticket_df.loc[:, ~ticket_df.columns.str.contains('^Unnamed')]\n",
    "        ticket_df.to_csv(f'Temporary_File_{date_str}.csv', index=False)\n",
    "\n",
    "        df = pd.read_csv(f'Temporary_File_{date_str}.csv', index_col=False)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        stops_df_unique = stops_df[['STOP NAME', 'CENTROID NUMBERING']].drop_duplicates(subset='STOP NAME')\n",
    "\n",
    "        df = df.merge(stops_df_unique, left_on='source', right_on='STOP NAME', how='left')\n",
    "        df = df.rename(columns={'CENTROID NUMBERING': 'source_zonal_centroid_number'})\n",
    "        df = df.drop(columns=['STOP NAME'])\n",
    "        cols = df.columns.tolist()\n",
    "        source_idx = cols.index('source_stop_no')\n",
    "        cols.insert(source_idx + 1, cols.pop(cols.index('source_zonal_centroid_number')))\n",
    "        df = df[cols]\n",
    "\n",
    "        df = df.merge(stops_df_unique, left_on='destination', right_on='STOP NAME', how='left')\n",
    "        df = df.rename(columns={'CENTROID NUMBERING': 'destination_zonal_centroid_number'})\n",
    "        df = df.drop(columns=['STOP NAME'])\n",
    "        cols = df.columns.tolist()\n",
    "        destination_idx = cols.index('destination_stop_no')\n",
    "        cols.insert(destination_idx + 1, cols.pop(cols.index('destination_zonal_centroid_number')))\n",
    "        df = df[cols]\n",
    "\n",
    "        df.to_csv(f'{folder_name}/Final_Ticket_data_2024-07-{date_str}.csv', index=False)\n",
    "        print(f\"File saved to {folder_name}/Final_Ticket_data_2024-07-{date_str}.csv\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for 2024-07-{date_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192211aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files for 2024-07-01\n",
      "Processed files for 2024-07-02\n",
      "Processed files for 2024-07-03\n",
      "Processed files for 2024-07-04\n",
      "Processed files for 2024-07-05\n",
      "Processed files for 2024-07-06\n",
      "Processed files for 2024-07-07\n",
      "Processed files for 2024-07-08\n",
      "Processed files for 2024-07-09\n",
      "Processed files for 2024-07-10\n",
      "Processed files for 2024-07-11\n",
      "Processed files for 2024-07-12\n",
      "Processed files for 2024-07-13\n",
      "Processed files for 2024-07-14\n",
      "Processed files for 2024-07-15\n",
      "Processed files for 2024-07-16\n",
      "Processed files for 2024-07-17\n",
      "Processed files for 2024-07-18\n",
      "Processed files for 2024-07-19\n",
      "Processed files for 2024-07-20\n",
      "Processed files for 2024-07-21\n",
      "Processed files for 2024-07-22\n",
      "Processed files for 2024-07-23\n",
      "Processed files for 2024-07-24\n",
      "Processed files for 2024-07-25\n",
      "Processed files for 2024-07-26\n",
      "Processed files for 2024-07-27\n",
      "Processed files for 2024-07-28\n",
      "Processed files for 2024-07-29\n",
      "Processed files for 2024-07-30\n",
      "Processed files for 2024-07-31\n"
     ]
    }
   ],
   "source": [
    "#up-down missing values \n",
    "# Directories\n",
    "folder_name_odm = \"ODM_Files\"\n",
    "folder_name_od = \"UP_DOWN\"\n",
    "\n",
    "\n",
    "# Define all zonal centroids\n",
    "all_zonal_centroids = range(1, 3039)\n",
    "all_pairs = pd.MultiIndex.from_product(\n",
    "    [all_zonal_centroids, all_zonal_centroids],\n",
    "    names=['source_zonal_centroid_number', 'destination_zonal_centroid_number']\n",
    ")\n",
    "\n",
    "# Loop through all dates in the month\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"\n",
    "    try:\n",
    "        # Read the CSV file for the current date\n",
    "        df = pd.read_csv(f'./Latest_ETM_Data/Final_Ticket_data_2024-07-{date_str}.csv')\n",
    "\n",
    "        # Create the OD Matrix\n",
    "        grouped = df.groupby(['source_zonal_centroid_number', 'destination_zonal_centroid_number'])['passenger_count'].sum()\n",
    "        od_matrix = grouped.reindex(all_pairs, fill_value=0).unstack(fill_value=0)\n",
    "        od_matrix['Total Passengers'] = od_matrix.sum(axis=1)\n",
    "        od_matrix.to_csv(f'{folder_name_odm}/ODM-07-{date_str}.csv', index=False)\n",
    "\n",
    "        # Process for UP_DOWN file\n",
    "        cols = ['Trip_Origin', 'Trip_Destination', 'Direction_route']\n",
    "        df_filtered = df[cols]\n",
    "        result = df_filtered[df_filtered['Direction_route'].isna()].drop_duplicates()\n",
    "        result.to_csv(f'{folder_name_od}/UP_DOWN_{date_str}.csv', index=False)\n",
    "        \n",
    "        print(f\"Processed files for 2024-07-{date_str}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for 2024-07-{date_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ea9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bb1b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: UP_DOWN/UP_DOWN_01.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_02.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_03.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_04.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_05.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_06.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_07.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_08.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_09.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_10.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_11.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_12.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_13.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_14.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_15.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_16.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_17.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_18.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_19.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_20.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_21.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_22.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_23.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_24.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_25.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_26.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_27.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_28.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_29.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_30.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_31.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining the values and storing it in a dataframe\n",
    "folder_name = \"UP_DOWN\"\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"  # Format date as two digits (e.g., 01, 02, ..., 31)\n",
    "    file_path = f'{folder_name}/UP_DOWN_{date_str}.csv'\n",
    "    \n",
    "    if os.path.exists(file_path):  # Check if file exists\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "\n",
    "unique_df = combined_df.drop_duplicates()\n",
    "\n",
    "combined_file_path = f\"{folder_name}/Combined_UP_DOWN_{date_str}.csv\"\n",
    "unique_file_path = f\"{folder_name}/Unique_UP_DOWN.csv\"\n",
    "\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "unique_df.to_csv(unique_file_path, index=False)\n",
    "unique_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac063ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4f2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Stop_Numbering/stop_numbering_01.csv\n",
      "Saved: Stop_Numbering/stop_numbering_02.csv\n",
      "Saved: Stop_Numbering/stop_numbering_03.csv\n",
      "Saved: Stop_Numbering/stop_numbering_04.csv\n",
      "Saved: Stop_Numbering/stop_numbering_05.csv\n",
      "Saved: Stop_Numbering/stop_numbering_06.csv\n",
      "Saved: Stop_Numbering/stop_numbering_07.csv\n",
      "Saved: Stop_Numbering/stop_numbering_08.csv\n",
      "Saved: Stop_Numbering/stop_numbering_09.csv\n",
      "Saved: Stop_Numbering/stop_numbering_10.csv\n",
      "Saved: Stop_Numbering/stop_numbering_11.csv\n",
      "Saved: Stop_Numbering/stop_numbering_12.csv\n",
      "Saved: Stop_Numbering/stop_numbering_13.csv\n",
      "Saved: Stop_Numbering/stop_numbering_14.csv\n",
      "Saved: Stop_Numbering/stop_numbering_15.csv\n",
      "Saved: Stop_Numbering/stop_numbering_16.csv\n",
      "Saved: Stop_Numbering/stop_numbering_17.csv\n",
      "Saved: Stop_Numbering/stop_numbering_18.csv\n",
      "Saved: Stop_Numbering/stop_numbering_19.csv\n",
      "Saved: Stop_Numbering/stop_numbering_20.csv\n",
      "Saved: Stop_Numbering/stop_numbering_21.csv\n",
      "Saved: Stop_Numbering/stop_numbering_22.csv\n",
      "Saved: Stop_Numbering/stop_numbering_23.csv\n",
      "Saved: Stop_Numbering/stop_numbering_24.csv\n",
      "Saved: Stop_Numbering/stop_numbering_25.csv\n",
      "Saved: Stop_Numbering/stop_numbering_26.csv\n",
      "Saved: Stop_Numbering/stop_numbering_27.csv\n",
      "Saved: Stop_Numbering/stop_numbering_28.csv\n",
      "Saved: Stop_Numbering/stop_numbering_29.csv\n",
      "Saved: Stop_Numbering/stop_numbering_30.csv\n",
      "Saved: Stop_Numbering/stop_numbering_31.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_name = \"Stop_Numbering\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02d}\"\n",
    "\n",
    "    # Read the CSV file for the given date\n",
    "    file_path = f'Latest_ETM_Data/Final_Ticket_data_2024-07-{date_str}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Select required columns\n",
    "    cols = ['source', 'source_stop_no', 'destination', 'destination_stop_no', 'Direction_route','Trip_Origin','Trip_Destination']\n",
    "    df = df[cols]\n",
    "\n",
    "    # Process source data\n",
    "    source_cols = ['source', 'source_stop_no', 'Direction_route','Trip_Origin','Trip_Destination']\n",
    "    source_df = df[source_cols]\n",
    "    source_df = source_df.rename(columns={'source': 'stop', 'source_stop_no': 'stop_no'})\n",
    "    source_df = source_df[source_df['stop_no'].isna()].drop_duplicates()\n",
    "\n",
    "    # Process destination data\n",
    "    destination_cols = ['destination', 'destination_stop_no', 'Direction_route','Trip_Origin','Trip_Destination']\n",
    "    destination_df = df[destination_cols]\n",
    "    destination_df = destination_df.rename(columns={'destination': 'stop', 'destination_stop_no': 'stop_no'})\n",
    "    destination_df = destination_df[destination_df['stop_no'].isna()].drop_duplicates()\n",
    "\n",
    "    # Combine source and destination DataFrames\n",
    "    combined_df = pd.concat([source_df, destination_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    combined_file_path = f\"{folder_name}/stop_numbering_{date_str}.csv\"\n",
    "    combined_df.to_csv(combined_file_path, index=False)\n",
    "    print(f\"Saved: {combined_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d1680f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: Stop_Numbering/stop_numbering_01.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_02.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_03.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_04.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_05.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_06.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_07.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_08.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_09.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_10.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_11.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_12.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_13.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_14.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_15.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_16.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_17.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_18.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_19.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_20.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_21.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_22.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_23.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_24.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_25.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_26.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_27.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_28.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_29.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_30.csv\n",
      "Reading file: Stop_Numbering/stop_numbering_31.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3053, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining the values and storing it in a dataframe\n",
    "#for all the dates from 01 to 31 - latest etm files \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_name = \"Stop_Numbering\"\n",
    "updated_df = pd.DataFrame()\n",
    "\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"  # Format date as two digits (e.g., 01, 02, ..., 31)\n",
    "    file_path = f\"{folder_name}/stop_numbering_{date_str}.csv\"\n",
    "    \n",
    "    if os.path.exists(file_path):  # Check if file exists\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        updated_df = pd.concat([updated_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "\n",
    "updated_df = updated_df.drop_duplicates()\n",
    "combined_file_path = f\"{folder_name}/Combined_stop_numbering_{date_str}.csv\"\n",
    "updated_df.to_csv(combined_file_path, index=False)\n",
    "\n",
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98452dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file_path = f\"{folder_name}/Combined_stop_numbering.csv\"\n",
    "updated_df=updated_df[updated_df['stop']!='ZERO VALUE TICKET']\n",
    "updated_df = updated_df[~updated_df['stop'].isna()]\n",
    "updated_df.to_csv(combined_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4e429a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(435, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c6dfa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop</th>\n",
       "      <th>stop_no</th>\n",
       "      <th>Direction_route</th>\n",
       "      <th>Trip_Origin</th>\n",
       "      <th>Trip_Destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EGMORE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>EGMORE NORTH R.S</td>\n",
       "      <td>ENNORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TONDIARPET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>EGMORE NORTH R.S</td>\n",
       "      <td>ENNORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TONDIARPET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>VALLALAR NAGAR</td>\n",
       "      <td>ENNORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TONDIARPET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>REDHILLS</td>\n",
       "      <td>ENNORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>THIRUVANMIYUR OR MARUDEESWARAR TEMPLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>THIRUVANMIYUR</td>\n",
       "      <td>ENNORE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67444</th>\n",
       "      <td>G.K.M.COLONY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>G.K.M.COLONY</td>\n",
       "      <td>BROADWAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69197</th>\n",
       "      <td>PERATHUR COLONY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UP</td>\n",
       "      <td>AVADI</td>\n",
       "      <td>KIZHANOOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69199</th>\n",
       "      <td>PERATHUR COLONY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UP</td>\n",
       "      <td>KIZHANOOR</td>\n",
       "      <td>AMBATTUR I.E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69538</th>\n",
       "      <td>GEMINI P H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>L.F.C</td>\n",
       "      <td>ANNA NAGAR WEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69969</th>\n",
       "      <td>KILAMBAKKAM WATER TANK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DOWN</td>\n",
       "      <td>THIRUVALLUR COLLECTOR OFFICE</td>\n",
       "      <td>AVADI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        stop  stop_no Direction_route  \\\n",
       "14                                    EGMORE      NaN            DOWN   \n",
       "15                                TONDIARPET      NaN            DOWN   \n",
       "17                                TONDIARPET      NaN            DOWN   \n",
       "28                                TONDIARPET      NaN            DOWN   \n",
       "34     THIRUVANMIYUR OR MARUDEESWARAR TEMPLE      NaN            DOWN   \n",
       "...                                      ...      ...             ...   \n",
       "67444                           G.K.M.COLONY      NaN            DOWN   \n",
       "69197                        PERATHUR COLONY      NaN              UP   \n",
       "69199                        PERATHUR COLONY      NaN              UP   \n",
       "69538                             GEMINI P H      NaN            DOWN   \n",
       "69969                 KILAMBAKKAM WATER TANK      NaN            DOWN   \n",
       "\n",
       "                        Trip_Origin Trip_Destination  \n",
       "14                 EGMORE NORTH R.S           ENNORE  \n",
       "15                 EGMORE NORTH R.S           ENNORE  \n",
       "17                   VALLALAR NAGAR           ENNORE  \n",
       "28                         REDHILLS           ENNORE  \n",
       "34                    THIRUVANMIYUR           ENNORE  \n",
       "...                             ...              ...  \n",
       "67444                  G.K.M.COLONY         BROADWAY  \n",
       "69197                         AVADI        KIZHANOOR  \n",
       "69199                     KIZHANOOR     AMBATTUR I.E  \n",
       "69538                         L.F.C  ANNA NAGAR WEST  \n",
       "69969  THIRUVALLUR COLLECTOR OFFICE            AVADI  \n",
       "\n",
       "[435 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ab474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
