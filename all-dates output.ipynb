{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ee6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# date=input(\"Enter the date: \")\n",
    "# df=pd.read_csv(f'./Latest_ETM_Data/Final_Ticket_data_2024-07-{date}.csv')\n",
    "# folder_name_odm=\"ODM_Files\"\n",
    "# folder_name_od=\"UP_DOWN\"\n",
    "\n",
    "\n",
    "# all_zonal_centroids = range(1, 3039)\n",
    "# all_pairs = pd.MultiIndex.from_product(\n",
    "#     [all_zonal_centroids, all_zonal_centroids],\n",
    "#     names=['source_zonal_centroid_number', 'destination_zonal_centroid_number']\n",
    "# )\n",
    "\n",
    "# grouped = df.groupby(['source_zonal_centroid_number', 'destination_zonal_centroid_number'])['passenger_count'].sum()\n",
    "# od_matrix = grouped.reindex(all_pairs, fill_value=0).unstack(fill_value=0)\n",
    "\n",
    "# od_matrix['Total Passengers'] = od_matrix.sum(axis=1)\n",
    "# od_matrix.to_csv(f'{folder_name_odm}/ODM-07-{date}.csv', index=False)\n",
    "\n",
    "\n",
    "# cols=['Trip_Origin','Trip_Destination','Direction_route']\n",
    "# df=df[cols]\n",
    "# result = df[df['Direction_route'].isna()].drop_duplicates()\n",
    "# result.to_csv(f'{folder_name_od}/UP_DOWN_{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909889d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fe3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8074f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3337b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da91ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vikhram\n",
    "\n",
    "# import pandas as pd\n",
    "# folder_name=\"Latest_ETM_Data\"\n",
    "# folder2_name=\"Match\"\n",
    "\n",
    "# #ETM file for a specific date\n",
    "# date=input(\"Enter the date: \")\n",
    "# ticket_df=pd.read_csv(f'ETM_Data/Ticket-Data-2024-07-{date}.csv')\n",
    "# ticket_df.head()\n",
    "\n",
    "\n",
    "# #Direction Route - UP and DOWN \n",
    "# route_df=pd.read_csv('final_routes_directions.csv')\n",
    "# ticket_df = ticket_df.merge(route_df[['Trip_Origin', 'Trip_Destination', 'Direction_Route']], \n",
    "#                             on=['Trip_Origin', 'Trip_Destination'], \n",
    "#                             how='left')\n",
    "\n",
    "# ticket_df.rename(columns={'Direction_Route': 'Direction_route'}, inplace=True)\n",
    "\n",
    "# ticket_df['source']=ticket_df['source'].str.upper().str.split().str.join(' ')\n",
    "# ticket_df['destination']=ticket_df['destination'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "# stops_df=pd.read_csv('latest_stops_centroid_numbering.csv')\n",
    "# stops_df['DIRECTION'] = stops_df['DIRECTION'].str.upper().str.split().str.join(' ')\n",
    "# stops_df['STOP NAME'] = stops_df['Stop Name'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #original and renamed stops mapping\n",
    "# final_stops_df= pd.read_csv('stops_mapping.csv')\n",
    "# mapping_stops = final_stops_df.set_index('Stops')['Renamed Stops'].to_dict()\n",
    "\n",
    "# ticket_df['source'] = ticket_df['source'].map(mapping_stops).fillna(ticket_df['source'])\n",
    "# ticket_df['destination'] = ticket_df['destination'].map(mapping_stops).fillna(ticket_df['destination'])\n",
    "\n",
    "\n",
    "# stops_df_unique = stops_df.drop_duplicates(subset=['STOP NAME', 'DIRECTION'])\n",
    "\n",
    "# def merge_and_insert(ticket_df, stops_df, stop_column, direction_column):\n",
    "#     ticket_df = ticket_df.merge(\n",
    "#         stops_df[['STOP.NO', 'STOP NAME', 'DIRECTION']],\n",
    "#         how='left',\n",
    "#         left_on=[stop_column, direction_column],\n",
    "#         right_on=['STOP NAME', 'DIRECTION']\n",
    "#     )\n",
    "\n",
    "#     # Rename and extract necessary columns\n",
    "#     stop_no_column = f\"{stop_column}_stop_no\"\n",
    "#     ticket_df = ticket_df.rename(columns={'STOP.NO': stop_no_column}).drop(columns=['STOP NAME', 'DIRECTION'])\n",
    "    \n",
    "#     stop_no = ticket_df.pop(stop_no_column)\n",
    "#     stop_index = ticket_df.columns.get_loc(stop_column) + 1  \n",
    "#     ticket_df.insert(stop_index, stop_no_column, stop_no)\n",
    "\n",
    "#     return ticket_df\n",
    "\n",
    "# ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'source', 'Direction_route')\n",
    "# ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'destination', 'Direction_route')\n",
    "\n",
    "# # Filter rows where DIRECTION is NaN\n",
    "# nodirection_stops= stops_df[stops_df['DIRECTION'].isna()]\n",
    "# nodirection_stops\n",
    "\n",
    "# # Unique stops without considering DIRECTION\n",
    "# nodirection_stops_unique = stops_df[stops_df['DIRECTION'].isna()].drop_duplicates(subset=['STOP NAME'])\n",
    "\n",
    "# def update_stop_no(ticket_df, stops_df, stop_column, stop_no_column):\n",
    "#     # Merge based on 'STOP NAME' only\n",
    "#     updated_df = ticket_df.merge(\n",
    "#         stops_df[['STOP.NO', 'STOP NAME']],\n",
    "#         how='left',\n",
    "#         left_on=stop_column,\n",
    "#         right_on='STOP NAME'\n",
    "#     )\n",
    "    \n",
    "#     # Only update rows where stop_no_column is NaN\n",
    "#     ticket_df.loc[ticket_df[stop_no_column].isna(), stop_no_column] = updated_df['STOP.NO']\n",
    "#     return ticket_df\n",
    "\n",
    "# ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'source', 'source_stop_no')\n",
    "# ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'destination', 'destination_stop_no')\n",
    "\n",
    "\n",
    "# ticket_df = ticket_df.loc[:, ~ticket_df.columns.str.contains('^Unnamed')]\n",
    "# ticket_df.to_csv(f'Temporary_File_{date}.csv')\n",
    "\n",
    "# df=pd.read_csv(f'Temporary_File_{date}.csv',index_col=False)\n",
    "# df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# source_list=ticket_df['source'].str.upper().unique()\n",
    "# destination_list=ticket_df['destination'].str.upper().unique()\n",
    "\n",
    "# stop_list=stops_df['STOP NAME'].unique()\n",
    "# source_arr=[]\n",
    "# source_count=0\n",
    "\n",
    "\n",
    "# combined_unmatched_stops=source_arr+dest_arr\n",
    "# unique_unmatched_stops=list(set(combined_unmatched_stops))\n",
    "# unique_unmatched_stops = [name for name in unique_unmatched_stops if pd.notna(name) and name.strip()]\n",
    "\n",
    "# unique_unmatched_df = pd.DataFrame(unique_unmatched_stops, columns=['unique_unmatched_stops'])\n",
    "# unique_unmatched_df = unique_unmatched_df.sort_values(by='unique_unmatched_stops').reset_index(drop=True)\n",
    "# unique_unmatched_df.to_csv(f'{folder2_name}/unique_unmatched_stops_{date}.csv', index=False)\n",
    "\n",
    "# stops_df=pd.read_csv('latest_stops_centroid_numbering.csv')\n",
    "# stops_df['STOP NAME'] = stops_df['Stop Name'].str.upper().str.split().str.join(' ')\n",
    "# stops_df['DIRECTION'] = stops_df['DIRECTION'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "\n",
    "# stops_df_unique = stops_df[['STOP NAME', 'CENTROID NUMBERING']].drop_duplicates(subset='STOP NAME')\n",
    "\n",
    "# df = df.merge(stops_df_unique, \n",
    "#               left_on='source', \n",
    "#               right_on='STOP NAME', \n",
    "#               how='left')\n",
    "\n",
    "# df = df.rename(columns={'CENTROID NUMBERING': 'source_zonal_centroid_number'})\n",
    "# df = df.drop(columns=['STOP NAME'])\n",
    "# cols = df.columns.tolist()\n",
    "# source_idx = cols.index('source_stop_no')\n",
    "# cols.insert(source_idx + 1, cols.pop(cols.index('source_zonal_centroid_number')))\n",
    "# df = df[cols]\n",
    "\n",
    "# stops_df_unique = stops_df[['STOP NAME', 'CENTROID NUMBERING']].drop_duplicates(subset='STOP NAME')\n",
    "\n",
    "# df = df.merge(stops_df_unique, \n",
    "#               left_on='destination', \n",
    "#               right_on='STOP NAME', \n",
    "#               how='left')\n",
    "\n",
    "# df = df.rename(columns={'CENTROID NUMBERING': 'destination_zonal_centroid_number'})\n",
    "# df = df.drop(columns=['STOP NAME'])\n",
    "\n",
    "# cols = df.columns.tolist()\n",
    "# destination_idx = cols.index('destination_stop_no')\n",
    "# cols.insert(destination_idx + 1, cols.pop(cols.index('destination_zonal_centroid_number')))\n",
    "# df = df[cols]\n",
    "# df.to_csv(f'{folder_name}/Final_Ticket_data_2024-07-{date}.csv', index=False)\n",
    "# print(f\"File saved to {folder_name}/Final_Ticket_data_2024-07-{date}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c7bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8570d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 2024-07-01...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-01.csv\n",
      "Processing for 2024-07-02...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-02.csv\n",
      "Processing for 2024-07-03...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-03.csv\n",
      "Processing for 2024-07-04...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-04.csv\n",
      "Processing for 2024-07-05...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-05.csv\n",
      "Processing for 2024-07-06...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-06.csv\n",
      "Processing for 2024-07-07...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-07.csv\n",
      "Processing for 2024-07-08...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-08.csv\n",
      "Processing for 2024-07-09...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-09.csv\n",
      "Processing for 2024-07-10...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-10.csv\n",
      "Processing for 2024-07-11...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-11.csv\n",
      "Processing for 2024-07-12...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-12.csv\n",
      "Processing for 2024-07-13...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-13.csv\n",
      "Processing for 2024-07-14...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-14.csv\n",
      "Processing for 2024-07-15...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-15.csv\n",
      "Processing for 2024-07-16...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-16.csv\n",
      "Processing for 2024-07-17...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-17.csv\n",
      "Processing for 2024-07-18...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-18.csv\n",
      "Processing for 2024-07-19...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-19.csv\n",
      "Processing for 2024-07-20...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-20.csv\n",
      "Processing for 2024-07-21...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-21.csv\n",
      "Processing for 2024-07-22...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-22.csv\n",
      "Processing for 2024-07-23...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-23.csv\n",
      "Processing for 2024-07-24...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-24.csv\n",
      "Processing for 2024-07-25...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-25.csv\n",
      "Processing for 2024-07-26...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-26.csv\n",
      "Processing for 2024-07-27...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-27.csv\n",
      "Processing for 2024-07-28...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-28.csv\n",
      "Processing for 2024-07-29...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-29.csv\n",
      "Processing for 2024-07-30...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-30.csv\n",
      "Processing for 2024-07-31...\n",
      "File saved to Latest_ETM_Data/Final_Ticket_data_2024-07-31.csv\n"
     ]
    }
   ],
   "source": [
    "#for all the dates from 01 to 31 - latest etm files \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directories\n",
    "folder_name = \"Latest_ETM_Data\"\n",
    "folder2_name = \"Match\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "os.makedirs(folder2_name, exist_ok=True)\n",
    "\n",
    "# Loop through all dates in the month\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"  # Format date as two digits (e.g., 01, 02, ..., 31)\n",
    "    try:\n",
    "        print(f\"Processing for 2024-07-{date_str}...\")\n",
    "        \n",
    "        # Load Ticket data for the current date\n",
    "        ticket_df = pd.read_csv(f'ETM_Data/Ticket-Data-2024-07-{date_str}.csv')\n",
    "\n",
    "        # Load Direction Route data\n",
    "        route_df = pd.read_csv('final_routes_directions.csv')\n",
    "        ticket_df = ticket_df.merge(route_df[['Trip_Origin', 'Trip_Destination', 'Direction_Route']], \n",
    "                                    on=['Trip_Origin', 'Trip_Destination'], \n",
    "                                    how='left')\n",
    "        ticket_df.rename(columns={'Direction_Route': 'Direction_route'}, inplace=True)\n",
    "\n",
    "        # Process stop names and directions\n",
    "        ticket_df['source'] = ticket_df['source'].str.upper().str.split().str.join(' ')\n",
    "        ticket_df['destination'] = ticket_df['destination'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "        stops_df = pd.read_csv('latest_stops_centroid_numbering.csv')\n",
    "        stops_df['DIRECTION'] = stops_df['DIRECTION'].str.upper().str.split().str.join(' ')\n",
    "        stops_df['STOP NAME'] = stops_df['Stop Name'].str.upper().str.split().str.join(' ')\n",
    "\n",
    "        # Map original and renamed stops\n",
    "        final_stops_df = pd.read_csv('stops_mapping.csv')\n",
    "        mapping_stops = final_stops_df.set_index('Stops')['Renamed Stops'].to_dict()\n",
    "\n",
    "        ticket_df['source'] = ticket_df['source'].map(mapping_stops).fillna(ticket_df['source'])\n",
    "        ticket_df['destination'] = ticket_df['destination'].map(mapping_stops).fillna(ticket_df['destination'])\n",
    "\n",
    "        stops_df_unique = stops_df.drop_duplicates(subset=['STOP NAME', 'DIRECTION'])\n",
    "\n",
    "        # Define merge_and_insert function\n",
    "        def merge_and_insert(ticket_df, stops_df, stop_column, direction_column):\n",
    "            ticket_df = ticket_df.merge(\n",
    "                stops_df[['STOP.NO', 'STOP NAME', 'DIRECTION']],\n",
    "                how='left',\n",
    "                left_on=[stop_column, direction_column],\n",
    "                right_on=['STOP NAME', 'DIRECTION']\n",
    "            )\n",
    "            stop_no_column = f\"{stop_column}_stop_no\"\n",
    "            ticket_df = ticket_df.rename(columns={'STOP.NO': stop_no_column}).drop(columns=['STOP NAME', 'DIRECTION'])\n",
    "            stop_no = ticket_df.pop(stop_no_column)\n",
    "            stop_index = ticket_df.columns.get_loc(stop_column) + 1  \n",
    "            ticket_df.insert(stop_index, stop_no_column, stop_no)\n",
    "            return ticket_df\n",
    "\n",
    "        # Insert stop numbers\n",
    "        ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'source', 'Direction_route')\n",
    "        ticket_df = merge_and_insert(ticket_df, stops_df_unique, 'destination', 'Direction_route')\n",
    "\n",
    "        # Process unmatched stops\n",
    "        nodirection_stops_unique = stops_df[stops_df['DIRECTION'].isna()].drop_duplicates(subset=['STOP NAME'])\n",
    "\n",
    "        def update_stop_no(ticket_df, stops_df, stop_column, stop_no_column):\n",
    "            updated_df = ticket_df.merge(\n",
    "                stops_df[['STOP.NO', 'STOP NAME']],\n",
    "                how='left',\n",
    "                left_on=stop_column,\n",
    "                right_on='STOP NAME'\n",
    "            )\n",
    "            ticket_df.loc[ticket_df[stop_no_column].isna(), stop_no_column] = updated_df['STOP.NO']\n",
    "            return ticket_df\n",
    "\n",
    "        ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'source', 'source_stop_no')\n",
    "        ticket_df = update_stop_no(ticket_df, nodirection_stops_unique, 'destination', 'destination_stop_no')\n",
    "\n",
    "        ticket_df = ticket_df.loc[:, ~ticket_df.columns.str.contains('^Unnamed')]\n",
    "        ticket_df.to_csv(f'Temporary_File_{date_str}.csv', index=False)\n",
    "\n",
    "        df = pd.read_csv(f'Temporary_File_{date_str}.csv', index_col=False)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "        stops_df_unique = stops_df[['STOP NAME', 'CENTROID NUMBERING']].drop_duplicates(subset='STOP NAME')\n",
    "\n",
    "        df = df.merge(stops_df_unique, left_on='source', right_on='STOP NAME', how='left')\n",
    "        df = df.rename(columns={'CENTROID NUMBERING': 'source_zonal_centroid_number'})\n",
    "        df = df.drop(columns=['STOP NAME'])\n",
    "        cols = df.columns.tolist()\n",
    "        source_idx = cols.index('source_stop_no')\n",
    "        cols.insert(source_idx + 1, cols.pop(cols.index('source_zonal_centroid_number')))\n",
    "        df = df[cols]\n",
    "\n",
    "        df = df.merge(stops_df_unique, left_on='destination', right_on='STOP NAME', how='left')\n",
    "        df = df.rename(columns={'CENTROID NUMBERING': 'destination_zonal_centroid_number'})\n",
    "        df = df.drop(columns=['STOP NAME'])\n",
    "        cols = df.columns.tolist()\n",
    "        destination_idx = cols.index('destination_stop_no')\n",
    "        cols.insert(destination_idx + 1, cols.pop(cols.index('destination_zonal_centroid_number')))\n",
    "        df = df[cols]\n",
    "\n",
    "        df.to_csv(f'{folder_name}/Final_Ticket_data_2024-07-{date_str}.csv', index=False)\n",
    "        print(f\"File saved to {folder_name}/Final_Ticket_data_2024-07-{date_str}.csv\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for 2024-07-{date_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "192211aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files for 2024-07-01\n",
      "Processed files for 2024-07-02\n",
      "Processed files for 2024-07-03\n",
      "Processed files for 2024-07-04\n",
      "Processed files for 2024-07-05\n",
      "Processed files for 2024-07-06\n",
      "Processed files for 2024-07-07\n",
      "Processed files for 2024-07-08\n",
      "Processed files for 2024-07-09\n",
      "Processed files for 2024-07-10\n",
      "Processed files for 2024-07-11\n",
      "Processed files for 2024-07-12\n",
      "Processed files for 2024-07-13\n",
      "Processed files for 2024-07-14\n",
      "Processed files for 2024-07-15\n",
      "Processed files for 2024-07-16\n",
      "Processed files for 2024-07-17\n",
      "Processed files for 2024-07-18\n",
      "Processed files for 2024-07-19\n",
      "Processed files for 2024-07-20\n",
      "Processed files for 2024-07-21\n",
      "Processed files for 2024-07-22\n",
      "Processed files for 2024-07-23\n",
      "Processed files for 2024-07-24\n",
      "Processed files for 2024-07-25\n",
      "Processed files for 2024-07-26\n",
      "Processed files for 2024-07-27\n",
      "Processed files for 2024-07-28\n",
      "Processed files for 2024-07-29\n",
      "Processed files for 2024-07-30\n",
      "Processed files for 2024-07-31\n"
     ]
    }
   ],
   "source": [
    "#up-down missing values \n",
    "# Directories\n",
    "folder_name_odm = \"ODM_Files\"\n",
    "folder_name_od = \"UP_DOWN\"\n",
    "\n",
    "\n",
    "# Define all zonal centroids\n",
    "all_zonal_centroids = range(1, 3039)\n",
    "all_pairs = pd.MultiIndex.from_product(\n",
    "    [all_zonal_centroids, all_zonal_centroids],\n",
    "    names=['source_zonal_centroid_number', 'destination_zonal_centroid_number']\n",
    ")\n",
    "\n",
    "# Loop through all dates in the month\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"\n",
    "    try:\n",
    "        # Read the CSV file for the current date\n",
    "        df = pd.read_csv(f'./Latest_ETM_Data/Final_Ticket_data_2024-07-{date_str}.csv')\n",
    "\n",
    "        # Create the OD Matrix\n",
    "        grouped = df.groupby(['source_zonal_centroid_number', 'destination_zonal_centroid_number'])['passenger_count'].sum()\n",
    "        od_matrix = grouped.reindex(all_pairs, fill_value=0).unstack(fill_value=0)\n",
    "        od_matrix['Total Passengers'] = od_matrix.sum(axis=1)\n",
    "        od_matrix.to_csv(f'{folder_name_odm}/ODM-07-{date_str}.csv', index=False)\n",
    "\n",
    "        # Process for UP_DOWN file\n",
    "        cols = ['Trip_Origin', 'Trip_Destination', 'Direction_route']\n",
    "        df_filtered = df[cols]\n",
    "        result = df_filtered[df_filtered['Direction_route'].isna()].drop_duplicates()\n",
    "        result.to_csv(f'{folder_name_od}/UP_DOWN_{date_str}.csv', index=False)\n",
    "        \n",
    "        print(f\"Processed files for 2024-07-{date_str}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for 2024-07-{date_str}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ea9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bb1b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: UP_DOWN/UP_DOWN_01.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_02.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_03.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_04.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_05.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_06.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_07.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_08.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_09.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_10.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_11.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_12.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_13.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_14.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_15.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_16.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_17.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_18.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_19.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_20.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_21.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_22.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_23.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_24.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_25.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_26.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_27.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_28.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_29.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_30.csv\n",
      "Reading file: UP_DOWN/UP_DOWN_31.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining the values and storing it in a dataframe\n",
    "folder_name = \"UP_DOWN\"\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"  # Format date as two digits (e.g., 01, 02, ..., 31)\n",
    "    file_path = f'{folder_name}/UP_DOWN_{date_str}.csv'\n",
    "    \n",
    "    if os.path.exists(file_path):  # Check if file exists\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        combined_df = pd.concat([combined_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "\n",
    "unique_df = combined_df.drop_duplicates()\n",
    "\n",
    "combined_file_path = f\"{folder_name}/Combined_UP_DOWN_{date_str}.csv\"\n",
    "unique_file_path = f\"{folder_name}/Unique_UP_DOWN.csv\"\n",
    "\n",
    "combined_df.to_csv(combined_file_path, index=False)\n",
    "unique_df.to_csv(unique_file_path, index=False)\n",
    "unique_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac063ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4f2a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Stop_Numbering/stop_numbering_01.csv\n",
      "Saved: Stop_Numbering/stop_numbering_02.csv\n",
      "Saved: Stop_Numbering/stop_numbering_03.csv\n",
      "Saved: Stop_Numbering/stop_numbering_04.csv\n",
      "Saved: Stop_Numbering/stop_numbering_05.csv\n",
      "Saved: Stop_Numbering/stop_numbering_06.csv\n",
      "Saved: Stop_Numbering/stop_numbering_07.csv\n",
      "Saved: Stop_Numbering/stop_numbering_08.csv\n",
      "Saved: Stop_Numbering/stop_numbering_09.csv\n",
      "Saved: Stop_Numbering/stop_numbering_10.csv\n",
      "Saved: Stop_Numbering/stop_numbering_11.csv\n",
      "Saved: Stop_Numbering/stop_numbering_12.csv\n",
      "Saved: Stop_Numbering/stop_numbering_13.csv\n",
      "Saved: Stop_Numbering/stop_numbering_14.csv\n",
      "Saved: Stop_Numbering/stop_numbering_15.csv\n",
      "Saved: Stop_Numbering/stop_numbering_16.csv\n",
      "Saved: Stop_Numbering/stop_numbering_17.csv\n",
      "Saved: Stop_Numbering/stop_numbering_18.csv\n",
      "Saved: Stop_Numbering/stop_numbering_19.csv\n",
      "Saved: Stop_Numbering/stop_numbering_20.csv\n",
      "Saved: Stop_Numbering/stop_numbering_21.csv\n",
      "Saved: Stop_Numbering/stop_numbering_22.csv\n",
      "Saved: Stop_Numbering/stop_numbering_23.csv\n",
      "Saved: Stop_Numbering/stop_numbering_24.csv\n",
      "Saved: Stop_Numbering/stop_numbering_25.csv\n",
      "Saved: Stop_Numbering/stop_numbering_26.csv\n",
      "Saved: Stop_Numbering/stop_numbering_27.csv\n",
      "Saved: Stop_Numbering/stop_numbering_28.csv\n",
      "Saved: Stop_Numbering/stop_numbering_29.csv\n",
      "Saved: Stop_Numbering/stop_numbering_30.csv\n",
      "Saved: Stop_Numbering/stop_numbering_31.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_name = \"Stop_Numbering\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02d}\"\n",
    "\n",
    "    # Read the CSV file for the given date\n",
    "    file_path = f'Latest_ETM_Data/Final_Ticket_data_2024-07-{date_str}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Select required columns\n",
    "    cols = ['source', 'source_stop_no', 'destination', 'destination_stop_no', 'Direction_route']\n",
    "    df = df[cols]\n",
    "\n",
    "    # Process source data\n",
    "    source_cols = ['source', 'source_stop_no', 'Direction_route']\n",
    "    source_df = df[source_cols]\n",
    "    source_df = source_df.rename(columns={'source': 'stop', 'source_stop_no': 'stop_no'})\n",
    "    source_df = source_df[source_df['stop_no'].isna()].drop_duplicates()\n",
    "\n",
    "    # Process destination data\n",
    "    destination_cols = ['destination', 'destination_stop_no', 'Direction_route']\n",
    "    destination_df = df[destination_cols]\n",
    "    destination_df = destination_df.rename(columns={'destination': 'stop', 'destination_stop_no': 'stop_no'})\n",
    "    destination_df = destination_df[destination_df['stop_no'].isna()].drop_duplicates()\n",
    "\n",
    "    # Combine source and destination DataFrames\n",
    "    combined_df = pd.concat([source_df, destination_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    combined_file_path = f\"{folder_name}/stop_numbering_{date_str}.csv\"\n",
    "    combined_df.to_csv(combined_file_path, index=False)\n",
    "    print(f\"Saved: {combined_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1680f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-01.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-02.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-03.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-04.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-05.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-06.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-07.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-08.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-09.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-10.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-11.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-12.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-13.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-14.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-15.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-16.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-17.csv\n",
      "Reading file: Latest_ETM_Data/Final_Ticket_data_2024-07-18.csv\n"
     ]
    }
   ],
   "source": [
    "#combining the values and storing it in a dataframe\n",
    "#for all the dates from 01 to 31 - latest etm files \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "folder_name = \"Stop_Numbering\"\n",
    "updated_df = pd.DataFrame()\n",
    "\n",
    "for date in range(1, 32):\n",
    "    date_str = f\"{date:02}\"  # Format date as two digits (e.g., 01, 02, ..., 31)\n",
    "    file_path = f'Latest_ETM_Data/Final_Ticket_data_2024-07-{date_str}.csv'\n",
    "    \n",
    "    if os.path.exists(file_path):  # Check if file exists\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        updated_df = pd.concat([updated_df, temp_df], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found for 2024-07-{date_str}, skipping.\")\n",
    "\n",
    "updated_df = updated_df.drop_duplicates()\n",
    "combined_file_path = f\"{folder_name}/Combined_stop_numbering_{date_str}.csv\"\n",
    "updated_df.to_csv(combined_file_path, index=False)\n",
    "\n",
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98452dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e429a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6dfa2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
